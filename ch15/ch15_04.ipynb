{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be28c45c",
   "metadata": {},
   "source": [
    "# 「テキスト」のエピステモロジー"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2635c9e5",
   "metadata": {},
   "source": [
    "## BoW(Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238506ef-fc7e-4b82-b39d-776f64e34613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15・4・1\n",
    "\n",
    "# シェークスピアのハムレットとマーロウのフォースタス博士の\n",
    "# 戯曲テキストを読み込む(出典 : プロジェクト・グーテンベルグ)\n",
    "\n",
    "import requests\n",
    "\n",
    "def get_text(url):\n",
    "    # urlからテキストを読み込む\n",
    "    r = requests.get(url)\n",
    "    return r.text\n",
    "\n",
    "BASE_URL = \"https://raw.githubusercontent.com/\"\n",
    "BASE_URL += \"shibats/minpy_5th/refs/heads/main/ch15/\"\n",
    "HAMLET_URL = BASE_URL + \"hamlet.txt\"\n",
    "DOCTOR_FAUSTUS_URL = BASE_URL + \"doctor_faustus.txt\"\n",
    "\n",
    "sha_text = get_text(HAMLET_URL)\n",
    "mar_text = get_text(DOCTOR_FAUSTUS_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5d724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15・4・2\n",
    "# 戯曲のテキストのうち余分な記号を取り除き，単語に分割\n",
    "import string\n",
    "\n",
    "def tokenize(text):\n",
    "    # 単語リスト作成関数\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return text.split()\n",
    "\n",
    "sha_words = tokenize(sha_text)\n",
    "mar_words = tokenize(mar_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd67125-48c8-4d27-b3dc-4410ae88b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15・4・3\n",
    "# counter.Counterを使って，単語ごとのの出現数を数える\n",
    "from collections import Counter\n",
    "# 上位単語20件を取得\n",
    "sha_cnt = Counter(sha_words).most_common(20)\n",
    "mar_cnt = Counter(mar_words).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3b754-f662-4e7c-92f7-b9209eb5be85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15・4・4\n",
    "# 単語の出現数をグラフに表示\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# グラフ表示\n",
    "s_words, s_counts = zip(*sha_cnt)\n",
    "m_words, m_counts = zip(*mar_cnt)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "axes[0].bar(s_words, s_counts, color='skyblue')\n",
    "axes[0].set_title(\"Hamlet (Shakespeare) Top 20\")\n",
    "\n",
    "axes[1].bar(m_words, m_counts, color='salmon')\n",
    "axes[1].set_title(\"Doctor Faustus (Marlowe) Top 20\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf501a1b",
   "metadata": {},
   "source": [
    "## N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93bfa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15・4・5\n",
    "def generate_ngrams(text, n):\n",
    "    # 文字列textからN-gramのデータを作り，set型にして返す\n",
    "    return set(text[i:i+n] for i in range(len(text) - n + 1))\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    # 2つのN-gramのJaccard係数を返す\n",
    "    if not set1 or not set2:\n",
    "        return 0.0\n",
    "    return len(set1 & set2) / len(set1 | set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fcf157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15・4・6\n",
    "query = \"吾輩は猫である\"\n",
    "\n",
    "corpus = [\n",
    "    \"私は猫である\",\n",
    "    \"吾輩は人である\",\n",
    "    \"僕は猫なんだよ\"\n",
    "]\n",
    "\n",
    "gery_2garm = generate_ngrams(query, 2)\n",
    "for sentence in corpus:\n",
    "    s_2gram = generate_ngrams(sentence, 2)\n",
    "    similarity = jaccard_similarity(gery_2garm, s_2gram)\n",
    "    print(sentence, similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ba899b",
   "metadata": {},
   "source": [
    "## ベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d180e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15・4・7  word2vecのデータをダウンロード(時間がかかります)\n",
    "!wget \"https://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/data/20170201.tar.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8efef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15・4・8  ダウンロードした圧縮ファイルを展開(とても時間がかかります)\n",
    "!tar jxvf 20170201.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773e340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15・4・9  # ライブラリをインストール\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43bfefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15・4・10 実行に時間がかかります\n",
    "import gensim\n",
    "import numpy as np\n",
    "# 東北大学の学習済みモデルをロード\n",
    "model_path = 'entity_vector/entity_vector.model.bin'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995c9680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15・4・11  単語を入力してベクトルの最初の10次元，類義語を表示\n",
    "\n",
    "word = input(\"日本語の単語を入力してください: \")\n",
    "\n",
    "# \n",
    "if word in model:\n",
    "    vec = model[word]\n",
    "    print(f\"\\n【{word}】のベクトル（最初の10次元）:\")\n",
    "    print(vec[:10])\n",
    "    print(\"\\n類似語（top5）:\")\n",
    "    print(model.most_similar(word, topn=5))\n",
    "else:\n",
    "    print(f\"単語「{word}」はモデルに存在しません。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9fa734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15・4・12\n",
    "# 猫と猿，犬，人の類似度を表示\n",
    "\n",
    "words = [\"猿\", \"犬\", \"人\"]\n",
    "the_word = \"猫\"\n",
    "for word in words:\n",
    "    similarity = model.similarity(the_word, word)\n",
    "    print(f\"{the_word}と{word}の類似度は{similarity}です。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4961ceee",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c2b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15・4・13  必要なライブラリをインストール\n",
    "!pip install fugashi\n",
    "!pip install unidic_lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd8d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15・4・14 実行に時間がかかります\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# モデルとトークナイザの読み込み\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
    "model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e594b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15・4・15\n",
    "# 文脈中の特定単語のベクトルを取得する関数\n",
    "def get_token_vector(sentence, target_word):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    token_tensor = torch.tensor([token_ids])\n",
    "\n",
    "    # 各トークンの位置を調べる（単語が分割される可能性に注意）\n",
    "    subwords = tokenizer.tokenize(target_word)\n",
    "    try:\n",
    "        start_idx = tokens.index(subwords[0])\n",
    "        end_idx = start_idx + len(subwords)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"単語「{target_word}」がトークン列に見つかりませんでした: {tokens}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(token_tensor)\n",
    "    embeddings = output.last_hidden_state[0]\n",
    "\n",
    "    # 対象単語に対応する subword ベクトルの平均を取る\n",
    "    word_vector = embeddings[start_idx:end_idx].mean(dim=0)\n",
    "    return word_vector.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8589dfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15・4・16\n",
    "# 類似度計算関数を定義\n",
    "def cosine_sim(v1, v2):\n",
    "    return cosine_similarity([v1], [v2])[0][0]\n",
    "\n",
    "# 文章を受け取り特定の単語の類似度を比較する関数を定義\n",
    "def compare_sentences(sentence1, sentence2, target_word):\n",
    "    vec1 = get_token_vector(sentence1, target_word)\n",
    "    vec2 = get_token_vector(sentence2, target_word)\n",
    "    return f\"文脈中の「{target}」の類似度: {cosine_sim(vec1, vec2):.4f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15・4・17\n",
    "sentence1 = \"Pythonはプログラミング言語です\"\n",
    "sentence2 = \"Pythonを使ってコードを書きます\"\n",
    "target = \"Python\"\n",
    "print(compare_sentences(sentence1, sentence2, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f36bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15・4・18\n",
    "# 文と対象単語のペア\n",
    "sentence1 = \"Pythonはプログラミング言語です\"\n",
    "sentence2 = \"Monty Pythonは楽しいテレビ番組です\"\n",
    "target = \"Python\"\n",
    "print(compare_sentences(sentence1, sentence2, target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
